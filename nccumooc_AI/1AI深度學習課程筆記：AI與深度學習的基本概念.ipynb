{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 課程簡介\n",
    "\n",
    "* 人工智慧核心概念為深度學習，而深度學習的核心技術為神經網路。\n",
    "* 有三種重要的模型：NN、CNN、RNN。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 AI人工智慧概念\n",
    "* AI就是問個好問題，將問題化為函數。\n",
    "* AI可以取代人？\n",
    "* 重申三種重要模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 AI的創建過程\n",
    "1. 問一個問題\n",
    "2. 化成函數形式(數學建模)\n",
    "3. 準備訓練資料\n",
    "4. 架構神經網路(如NN、CNN、RNN、強化學習與GAN)，調整參數並獲得函數\n",
    "5. 檢驗訓練結果，使用loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 可以用AI解決的問題\n",
    "* 例如一：想知道明日股市收盤價\n",
    "* 例如二：來年球員打擊數\n",
    "* 例如三：對話機器人(RNN可以用簡易方式建構出來)\n",
    "* 例如四：自動駕駛、自動玩遊戲\n",
    "* 例如五：創建新的字型\n",
    "\n",
    "##### 如何得到最佳的函數？每次訓練結果予以評分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 神經網路基本原理\n",
    "* AI的核心技術為神經網路。\n",
    "\n",
    "\n",
    "### 標準神經網路(Fully Connected Neural Networks)：簡稱為NN。\n",
    "  1. 1980年代火紅的模型\n",
    "  2. 輸入層-隱藏層(內含神經元串接)-輸出層\n",
    "  3. 神經元接受多個輸入，但僅送出一個輸出。\n",
    "  4. 輸入值(x)與權重(w)形成加權和，並加入調整值-偏值(bias)，最終形成總刺激(h)。\n",
    "  5. 激活函數(Activation function)：為避免函數僅為線性函數，因此需透過激活函數形成輸出。\n",
    "     * 主要三種激活函數：\n",
    "        - ReLU(最常使用且現在流動)：正值具有效果。\n",
    "        - Sigmoid(最常使用)：模擬神經元，依據輸入值大小進行調整，避免數值過度膨脹。\n",
    "        - Gaussian(不常使用)\n",
    "    ![網路](1_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 神經網路訓練法\n",
    "  - 使用既有資料進行訓練並修正，學習法稱為backpropagation。\n",
    "  - 固定神經網路的函數空間(神經網路結構、激活函數)，並調整權重與偏誤，每次調整即獲得一組參數。\n",
    "\n",
    "### Loss Function \n",
    "  希望得到\"最好\"的函數最接近目標函數。最好的判準為loss function最小。\n",
    "  ![最常用的loss fun](1_2.jpg)\n",
    "  * 透過輸出值與實際數值差距的平方和，並乘上1/2，形成上述最常用的L函數。\n",
    "\n",
    "### 如何調整函數？\n",
    "  ![調整函數](1_3.jpg)\n",
    "* **目標：調整權重與偏值，使得L()得到最小值**\n",
    "  1. 微分取得函數極值\n",
    "  2. 加上負號使得切線斜率向另一側修正(斜率為負向右，斜率為正向左)，逼近最小值。\n",
    "  3. 加上較小的learning rate，避免調整幅度過大。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 梯度下降法(gradient descent)\n",
    "\n",
    "* 調整多個參數：控制其他參數(為常數)，並只調整一個變數，形成只有一個變數的函數。換句話，此方法即是使用偏微分。\n",
    "    ![gradient](1_4.jpg)\n",
    "* 符號取代後，如下\n",
    "    ![gradient](1_5.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
